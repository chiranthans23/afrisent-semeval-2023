{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_DIR = '/home/admin/data/afrisent-semeval-2023'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Cbmi_mQ4k3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from -r starter_kit/requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from -r starter_kit/requirements.txt (line 2)) (1.21.6)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.7/site-packages (from -r starter_kit/requirements.txt (line 4)) (1.13.0)\n",
            "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from -r starter_kit/requirements.txt (line 5)) (1.7.3)\n",
            "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from -r starter_kit/requirements.txt (line 6)) (4.21.8)\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from -r starter_kit/requirements.txt (line 7)) (1.0.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.14.0-py3-none-any.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=1.8.0\n",
            "  Downloading datasets-2.7.0-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.6/451.6 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r starter_kit/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r starter_kit/requirements.txt (line 1)) (2022.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (4.11.4)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (2.28.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (21.3)\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.3->-r starter_kit/requirements.txt (line 4)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch>=1.3->-r starter_kit/requirements.txt (line 4)) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.3->-r starter_kit/requirements.txt (line 4)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch>=1.3->-r starter_kit/requirements.txt (line 4)) (8.5.0.96)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.3->-r starter_kit/requirements.txt (line 4)) (4.4.0)\n",
            "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3->-r starter_kit/requirements.txt (line 4)) (0.37.1)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3->-r starter_kit/requirements.txt (line 4)) (59.8.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->-r starter_kit/requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->-r starter_kit/requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate->-r starter_kit/requirements.txt (line 8)) (5.9.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (10.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (2022.10.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.8.3)\n",
            "Collecting dill<0.3.7\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (22.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.7.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers->-r starter_kit/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->-r starter_kit/requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (2022.9.24)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers->-r starter_kit/requirements.txt (line 3)) (3.10.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, xxhash, regex, filelock, dill, responses, multiprocess, huggingface-hub, transformers, datasets, accelerate, evaluate\n",
            "Successfully installed accelerate-0.14.0 datasets-2.7.0 dill-0.3.6 evaluate-0.3.0 filelock-3.8.0 huggingface-hub-0.11.0 multiprocess-0.70.14 regex-2022.10.31 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.24.0 xxhash-3.1.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.path.isdir(PROJECT_DIR):\n",
        "  #The requirements file should be in PROJECT_DIR\n",
        "  if os.path.isfile(os.path.join(PROJECT_DIR, 'starter_kit/requirements.txt')):\n",
        "    !pip install -r starter_kit/requirements.txt\n",
        "  else:\n",
        "    print('requirements.txt file not found')\n",
        "\n",
        "else:\n",
        "  print(\"Project directory not found, please check again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zszKhh2Ufb3"
      },
      "source": [
        "##d. Import libraries\n",
        "\n",
        "Import libraries below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8QIl420aUM1O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Please don not edit anything here\n",
        "languages = ['am', 'dz', 'ha', 'ig', 'ma', 'pcm', 'pt', 'sw', 'yo']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoRyMJMDJ7lF"
      },
      "source": [
        "#2) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QzoSbWC678Zm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory found.\n",
            "Creating directory to store formatted data.\n"
          ]
        }
      ],
      "source": [
        "# Training Data Paths\n",
        "\n",
        "TASK = 'SubtaskB'\n",
        "TRAINING_DATA_DIR = os.path.join(PROJECT_DIR, TASK)\n",
        "FORMATTED_TRAIN_DATA = os.path.join(TRAINING_DATA_DIR, 'formatted-train-data')\n",
        "\n",
        "if os.path.isdir(TRAINING_DATA_DIR):\n",
        "  print('Data directory found.')\n",
        "  if not os.path.isdir(FORMATTED_TRAIN_DATA):\n",
        "    print('Creating directory to store formatted data.')\n",
        "    os.mkdir(FORMATTED_TRAIN_DATA)\n",
        "else:\n",
        "  print(TRAINING_DATA_DIR + ' is not a valid directory or does not exist!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ssyIZOUJMrzM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/admin/data/afrisent-semeval-2023/SubtaskA/train\n",
            "ha Creating directory to store train, dev and test splits.\n",
            "kr Creating directory to store train, dev and test splits.\n",
            "ig Creating directory to store train, dev and test splits.\n",
            "pcm Creating directory to store train, dev and test splits.\n",
            "yo Creating directory to store train, dev and test splits.\n",
            "am Creating directory to store train, dev and test splits.\n",
            "pt Creating directory to store train, dev and test splits.\n",
            "formatted-train-data skipped!\n",
            "ma Creating directory to store train, dev and test splits.\n",
            "dz Creating directory to store train, dev and test splits.\n",
            "ts Creating directory to store train, dev and test splits.\n",
            "sw Creating directory to store train, dev and test splits.\n",
            "twi Creating directory to store train, dev and test splits.\n",
            "README.txt skipped!\n"
          ]
        }
      ],
      "source": [
        "%cd {TRAINING_DATA_DIR}\n",
        "\n",
        "training_files = os.listdir()\n",
        "\n",
        "if len(training_files) > 0:\n",
        "  for training_file in training_files:\n",
        "    if training_file.endswith('.tsv'):\n",
        "\n",
        "      data = training_file.split('_')[0]\n",
        "      if not os.path.isdir(os.path.join(FORMATTED_TRAIN_DATA, data)):\n",
        "        print(data, 'Creating directory to store train, dev and test splits.')\n",
        "        os.mkdir(os.path.join(FORMATTED_TRAIN_DATA, data))\n",
        "      \n",
        "      df = pd.read_csv(training_file, sep='\\t', names=['ID', 'text', 'label'], header=0)\n",
        "      df[['text', 'label']].to_csv(os.path.join(FORMATTED_TRAIN_DATA, data, 'train.tsv'), sep='\\t', index=False)\n",
        "    else:\n",
        "      print(training_file + ' skipped!')\n",
        "else:\n",
        "  print('No files are found in this directory!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S2Dup8GHl1Q"
      },
      "source": [
        "After running the code above, a new folder (called formated-train-data) with formated files is created in the \"datasets\" folder in the train sub-folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LgeVN_wXGrq"
      },
      "source": [
        "##b. <font color='red'>`(Optional) Creating Evaluation (Dev and Test) sets from the available training data`</font>\n",
        "\n",
        "You may wish to create train and evaluation (dev and test) sets from the training data provided. If you wish to do so, you can run any of the cells below`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APxVxL06lfux"
      },
      "source": [
        "###i. If you want to create both the Dev and Test sets, run this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aVq1Blz0YF2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory found.\n",
            "Creating directory to store train, dev and test splits.\n",
            "/home/admin/data/afrisent-semeval-2023/SubtaskA/train/formatted-train-data\n",
            "twi Creating directory to store train, dev and test splits.\n",
            "ma Creating directory to store train, dev and test splits.\n",
            "dz Creating directory to store train, dev and test splits.\n",
            "ig Creating directory to store train, dev and test splits.\n",
            "ha Creating directory to store train, dev and test splits.\n",
            "pt Creating directory to store train, dev and test splits.\n",
            "yo Creating directory to store train, dev and test splits.\n",
            "pcm Creating directory to store train, dev and test splits.\n",
            "am Creating directory to store train, dev and test splits.\n",
            "sw Creating directory to store train, dev and test splits.\n",
            "kr Creating directory to store train, dev and test splits.\n",
            "ts Creating directory to store train, dev and test splits.\n"
          ]
        }
      ],
      "source": [
        "if os.path.isdir(FORMATTED_TRAIN_DATA):\n",
        "  print('Data directory found.')\n",
        "  SPLITTED_DATA = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test')\n",
        "  if not os.path.isdir(SPLITTED_DATA):\n",
        "    print('Creating directory to store train, dev and test splits.')\n",
        "    os.mkdir(SPLITTED_DATA)\n",
        "else:\n",
        "  print(FORMATTED_TRAIN_DATA + ' is not a valid directory or does not exist!')\n",
        "\n",
        "%cd {FORMATTED_TRAIN_DATA}\n",
        "formatted_training_files = os.listdir()\n",
        "\n",
        "if len(formatted_training_files) > 0:\n",
        "  for data_name in formatted_training_files:\n",
        "    formatted_training_file = os.path.join(data_name, 'train.tsv')\n",
        "    if os.path.isfile(formatted_training_file):\n",
        "      labeled_tweets = pd.read_csv(formatted_training_file, sep='\\t', names=['text', 'label'], header=0)\n",
        "      train, dev, test = np.split(labeled_tweets.sample(frac=1, random_state=42), [int(.7*len(labeled_tweets)), int(.8*len(labeled_tweets))])\n",
        "\n",
        "      if not os.path.isdir(os.path.join(SPLITTED_DATA, data_name)):\n",
        "        print(data_name, 'Creating directory to store train, dev and test splits.')\n",
        "        os.mkdir(os.path.join(SPLITTED_DATA, data_name))\n",
        "\n",
        "      train.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'train.tsv'), sep='\\t', index=False)\n",
        "      dev.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'dev.tsv'), sep='\\t', index=False)\n",
        "      test.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name,'test.tsv'), sep='\\t', index=False)\n",
        "    else:\n",
        "      print(training_file + ' is not a supported file!')\n",
        "else:\n",
        "  print('No files are found in this directory!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZy_rzouJZFz"
      },
      "source": [
        "After running the code above, a new folder (called splitted-train-dev-test) with train-dev-test split is created in the \"datasets\" folder in the train sub-folder. Here, the train-dev-test split is 70/10/20\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4UObc2ql-Zd"
      },
      "source": [
        "###ii. If you want to create only the Dev set from the training data, please run this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tU24jW_gmFu9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory found.\n",
            "Creating directory to store train, dev and test splits.\n",
            "/home/admin/data/afrisent-semeval-2023/SubtaskA/train/formatted-train-data\n",
            "twi Creating directory to store train, dev and test splits.\n",
            "ma Creating directory to store train, dev and test splits.\n",
            "dz Creating directory to store train, dev and test splits.\n",
            "ig Creating directory to store train, dev and test splits.\n",
            "ha Creating directory to store train, dev and test splits.\n",
            "pt Creating directory to store train, dev and test splits.\n",
            "yo Creating directory to store train, dev and test splits.\n",
            "pcm Creating directory to store train, dev and test splits.\n",
            "am Creating directory to store train, dev and test splits.\n",
            "sw Creating directory to store train, dev and test splits.\n",
            "kr Creating directory to store train, dev and test splits.\n",
            "ts Creating directory to store train, dev and test splits.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if os.path.isdir(FORMATTED_TRAIN_DATA):\n",
        "  print('Data directory found.')\n",
        "  SPLITTED_DATA = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev')\n",
        "  if not os.path.isdir(SPLITTED_DATA):\n",
        "    print('Creating directory to store train, dev and test splits.')\n",
        "    os.mkdir(SPLITTED_DATA)\n",
        "else:\n",
        "  print(FORMATTED_TRAIN_DATA + ' is not a valid directory or does not exist!')\n",
        "\n",
        "%cd {FORMATTED_TRAIN_DATA}\n",
        "formatted_training_files = os.listdir()\n",
        "\n",
        "if len(formatted_training_files) > 0:\n",
        "  for data_name in formatted_training_files:\n",
        "    formatted_training_file = os.path.join(data_name, 'train.tsv')\n",
        "    if os.path.isfile(formatted_training_file):\n",
        "      labeled_tweets = pd.read_csv(formatted_training_file, sep='\\t', names=['text', 'label'], header=0)\n",
        "      train, dev = train_test_split(labeled_tweets, test_size=0.3)\n",
        "\n",
        "      if not os.path.isdir(os.path.join(SPLITTED_DATA, data_name)):\n",
        "        print(data_name, 'Creating directory to store train, dev and test splits.')\n",
        "        os.mkdir(os.path.join(SPLITTED_DATA, data_name))\n",
        "\n",
        "      train.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'train.tsv'), sep='\\t', index=False)\n",
        "      dev.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'dev.tsv'), sep='\\t', index=False)\n",
        "    else:\n",
        "      print(training_file + ' is not a supported file!')\n",
        "else:\n",
        "  print('No files are found in this directory!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usfr00QhKSnd"
      },
      "source": [
        "After running the code above, a new folder (called splitted-train-dev) with train-dev split is created in the \"datasets\" folder in the train sub-folder. Here, the train-dev split is 70/30\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDoyRlje3Rm7"
      },
      "source": [
        "#3) Training setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AaXec415s0f"
      },
      "source": [
        "##a. Set project parameters\n",
        "\n",
        "For a list of models that be used for fine-tuning, you can check [HERE](https://huggingface.co/models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M0TKIFrE5ybV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/admin/data/afrisent-semeval-2023\n",
            "Everything set. You can now start model training.\n"
          ]
        }
      ],
      "source": [
        "%cd {PROJECT_DIR}\n",
        "\n",
        "# Language to train sentiment classifier for\n",
        "LANGUAGE_CODE = 'ha'\n",
        "\n",
        "if LANGUAGE_CODE in languages:\n",
        "  # Model Training Parameters\n",
        "  MODEL_NAME_OR_PATH = 'Davlan/afro-xlmr-mini'\n",
        "  BATCH_SIZE = 32\n",
        "  LEARNING_RATE = 5e-5\n",
        "  NUMBER_OF_TRAINING_EPOCHS = 5\n",
        "  MAXIMUM_SEQUENCE_LENGTH = 128\n",
        "  SAVE_STEPS = -1\n",
        "\n",
        "  print('Everything set. You can now start model training.')\n",
        "\n",
        "else:\n",
        "  print(\"Invalid language code/Dataset not released. Please input any of the following released data\\n\\n\\t- 'am'\\n\\t- 'dz'\\n\\t- 'ha'\\n\\t- 'ig'\\n\\t- 'ma'\\n\\t- 'pcm'\\n\\t- 'pt'\\n\\t- 'sw'\\n\\t- 'yo'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2qcCQnU8dgQ"
      },
      "source": [
        "##b. Train the model\n",
        "\n",
        "In the section below, we provide three options: \n",
        "\n",
        "- 1) training model without any validation; \n",
        "- 2) training model with validation but without testing; \n",
        "- 3) training a model with validation and test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fEw-qcEhYnx"
      },
      "source": [
        "###i. Training on only Train set, without any evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kAgg2MmAhiiW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/16/2022 22:50:01 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/16/2022 22:50:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/home/admin/data/afrisent-semeval-2023/models/ha_no_eval/runs/Nov16_22-50-01_instance-1,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/home/admin/data/afrisent-semeval-2023/models/ha_no_eval,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/home/admin/data/afrisent-semeval-2023/models/ha_no_eval,\n",
            "save_on_each_node=False,\n",
            "save_steps=-1,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "['negative', 'neutral', 'positive']\n",
            "[INFO|configuration_utils.py:654] 2022-11-16 22:50:01,738 >> loading configuration file config.json from cache at /home/admin/.cache/huggingface/hub/models--Davlan--afro-xlmr-mini/snapshots/fda28c209984483d597e7388c80b6f6ae5af59b5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-16 22:50:01,741 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"Davlan/afro-xlmr-mini\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-16 22:50:01,847 >> loading file sentencepiece.bpe.model from cache at /home/admin/.cache/huggingface/hub/models--Davlan--afro-xlmr-mini/snapshots/fda28c209984483d597e7388c80b6f6ae5af59b5/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-16 22:50:01,847 >> loading file tokenizer.json from cache at /home/admin/.cache/huggingface/hub/models--Davlan--afro-xlmr-mini/snapshots/fda28c209984483d597e7388c80b6f6ae5af59b5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-16 22:50:01,847 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-16 22:50:01,847 >> loading file special_tokens_map.json from cache at /home/admin/.cache/huggingface/hub/models--Davlan--afro-xlmr-mini/snapshots/fda28c209984483d597e7388c80b6f6ae5af59b5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-16 22:50:01,847 >> loading file tokenizer_config.json from cache at /home/admin/.cache/huggingface/hub/models--Davlan--afro-xlmr-mini/snapshots/fda28c209984483d597e7388c80b6f6ae5af59b5/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2158] 2022-11-16 22:50:02,562 >> loading weights file pytorch_model.bin from cache at /home/admin/.cache/huggingface/hub/models--Davlan--afro-xlmr-mini/snapshots/fda28c209984483d597e7388c80b6f6ae5af59b5/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2599] 2022-11-16 22:50:03,685 >> Some weights of the model checkpoint at Davlan/afro-xlmr-mini were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2611] 2022-11-16 22:50:03,685 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-mini and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset: 100%|███████| 15/15 [00:01<00:00, 11.50ba/s]\n",
            "11/16/2022 22:50:05 - INFO - __main__ - Sample 10476 of the training set: {'text': \"@user Rabbana laa taj'alna fitnatallillazina amanu👏\", 'label': 2, '__index_level_0__': 10476, 'input_ids': [0, 1374, 65918, 178382, 1500, 21, 11, 11729, 25, 72950, 11177, 76, 91569, 2298, 27459, 14876, 34, 243404, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/16/2022 22:50:05 - INFO - __main__ - Sample 1824 of the training set: {'text': '@user Kay mu abun yayi yawa gashi govnati bata iya samarwa da muta ne kome 🙉 sede wahala👅😋😭😠', 'label': 0, '__index_level_0__': 1824, 'input_ids': [0, 1374, 65918, 26125, 842, 10, 17643, 127483, 151, 634, 914, 3767, 738, 18803, 118, 8336, 22520, 118399, 634, 48, 53664, 108, 71075, 6, 3, 19378, 259, 18033, 248218, 243584, 232773, 247586, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/16/2022 22:50:05 - INFO - __main__ - Sample 409 of the training set: {'text': \"@user Ta yaya za'ayi inzama mai ilimi?😭😭 Wallahi ya burgeni sosai, Mitewwww Wagga rayuwa akwai matsala yadace ace mutum ya yafe duniya ya koma ga Zalla domin samun rabo mai tsoka duniya da lahira🤦\\u200d♂️🤦\\u200d♂️\", 'label': 0, '__index_level_0__': 409, 'input_ids': [0, 1374, 65918, 1218, 151, 395, 80, 25, 53797, 23, 76272, 409, 17, 17975, 32, 232773, 232773, 23706, 24483, 151, 6818, 33781, 221, 27305, 4, 3574, 36550, 1574, 131497, 208, 203753, 192807, 2589, 20088, 117580, 329, 10, 329, 152170, 151, 151, 2242, 88206, 151, 17275, 914, 825, 1165, 15102, 99113, 136340, 409, 6, 51566, 161, 88206, 48, 55341, 11, 245777, 6, 228250, 15755, 245777, 6, 228250, 15755, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-16 22:50:06,942 >> The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1608] 2022-11-16 22:50:06,952 >> ***** Running training *****\n",
            "[INFO|trainer.py:1609] 2022-11-16 22:50:06,952 >>   Num examples = 14172\n",
            "[INFO|trainer.py:1610] 2022-11-16 22:50:06,952 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1611] 2022-11-16 22:50:06,952 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1612] 2022-11-16 22:50:06,952 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1613] 2022-11-16 22:50:06,952 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1614] 2022-11-16 22:50:06,952 >>   Total optimization steps = 2215\n",
            "[INFO|trainer.py:1616] 2022-11-16 22:50:06,953 >>   Number of trainable parameters = 117641859\n",
            "{'loss': 0.8441, 'learning_rate': 3.8713318284424384e-05, 'epoch': 1.13}        \n",
            "{'loss': 0.6654, 'learning_rate': 2.742663656884876e-05, 'epoch': 2.26}         \n",
            "{'loss': 0.5729, 'learning_rate': 1.613995485327314e-05, 'epoch': 3.39}         \n",
            "{'loss': 0.5085, 'learning_rate': 4.853273137697517e-06, 'epoch': 4.51}         \n",
            "100%|███████████████████████████████████████| 2215/2215 [09:18<00:00,  4.09it/s][INFO|trainer.py:1859] 2022-11-16 22:59:25,588 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 558.6369, 'train_samples_per_second': 126.844, 'train_steps_per_second': 3.965, 'train_loss': 0.6307296615152811, 'epoch': 5.0}\n",
            "100%|███████████████████████████████████████| 2215/2215 [09:18<00:00,  3.97it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-16 22:59:25,592 >> Saving model checkpoint to /home/admin/data/afrisent-semeval-2023/models/ha_no_eval\n",
            "[INFO|configuration_utils.py:447] 2022-11-16 22:59:25,593 >> Configuration saved in /home/admin/data/afrisent-semeval-2023/models/ha_no_eval/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-16 22:59:26,238 >> Model weights saved in /home/admin/data/afrisent-semeval-2023/models/ha_no_eval/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:59:26,239 >> tokenizer config file saved in /home/admin/data/afrisent-semeval-2023/models/ha_no_eval/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:59:26,239 >> Special tokens file saved in /home/admin/data/afrisent-semeval-2023/models/ha_no_eval/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.6307\n",
            "  train_runtime            = 0:09:18.63\n",
            "  train_samples            =      14172\n",
            "  train_samples_per_second =    126.844\n",
            "  train_steps_per_second   =      3.965\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'formatted-train-data', LANGUAGE_CODE)\n",
        "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE + '_no_eval')\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python starter_kit/run_textclass.py \\\n",
        "  --model_name_or_path {MODEL_NAME_OR_PATH} \\\n",
        "  --data_dir {DATA_DIR} \\\n",
        "  --do_train \\\n",
        "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "  --learning_rate {LEARNING_RATE} \\\n",
        "  --num_train_epochs {NUMBER_OF_TRAINING_EPOCHS} \\\n",
        "  --max_seq_length {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --output_dir {OUTPUT_DIR} \\\n",
        "  --save_steps {SAVE_STEPS}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZtt5lBBLPz1"
      },
      "source": [
        "As you may observe, the training loss is very large. As a start, you can tune the training parameters and model to get a competitive result. \n",
        "\n",
        "You can observe also, there is no validation metrics (e.g., accuracy, loss etc) since we are only training without validtaion "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpFSMaAzhS5Q"
      },
      "source": [
        "###ii. Training on only Train and Dev sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TMAD_iLhpgI"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev', LANGUAGE_CODE)\n",
        "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE + '_no_test')\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python starter_kit/run_textclass.py \\\n",
        "  --model_name_or_path {MODEL_NAME_OR_PATH} \\\n",
        "  --data_dir {DATA_DIR} \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "  --learning_rate {LEARNING_RATE} \\\n",
        "  --num_train_epochs {NUMBER_OF_TRAINING_EPOCHS} \\\n",
        "  --max_seq_length {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --output_dir {OUTPUT_DIR} \\\n",
        "  --save_steps {SAVE_STEPS}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3I2VJlCMJTN"
      },
      "source": [
        "Now, you can observe, there is evalidation metrics (e.g., accuracy, loss etc) since we are evaluating our model performance on the validation set we created from the \n",
        "training data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW99ncOhhDEm"
      },
      "source": [
        "###iii. Training with Train, Dev and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaVssq7a1uwk"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test', LANGUAGE_CODE)\n",
        "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE)\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python starter_kit/run_textclass.py \\\n",
        "  --model_name_or_path {MODEL_NAME_OR_PATH} \\\n",
        "  --data_dir {DATA_DIR} \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "  --learning_rate {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --num_train_epochs {NUMBER_OF_TRAINING_EPOCHS} \\\n",
        "  --max_seq_length {MAXIMUM_SEQUENCE_LENGTH} \\\n",
        "  --output_dir {OUTPUT_DIR} \\\n",
        "  --save_steps {SAVE_STEPS}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lLKHsrEPucP"
      },
      "source": [
        "Now that you trained your best model and find the best  parameters, you can submit your prediction on dev or test set on CodaLab competition page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9DeJD4CVmEl"
      },
      "source": [
        "#4) Submission\n",
        "\n",
        "- For submission after training your model, unlabeled tweets were provided for dev (development phase) and test (evaluation phase). \n",
        "\n",
        "- To generate their sentiment prediction, provide the path to the file containing the unlabeled tweets.\n",
        "\n",
        "**What the code does**\n",
        "1. Predicting sentiments of the unlabeled tweets (dev or test)\n",
        "2. Create a file in the submission format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYL3KcmoWXK1"
      },
      "outputs": [],
      "source": [
        "%cd {PROJECT_DIR}\n",
        "\n",
        "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE)\n",
        "FILE_NAME = os.path.join(PROJECT_DIR, TASK, 'dev', LANGUAGE_CODE + '_dev.tsv')\n",
        "TEXT_COLUMN = 'tweet'\n",
        "\n",
        "!python starter_kit/run_predict.py \\\n",
        "  --model_path {OUTPUT_DIR} \\\n",
        "  --file_name {FILE_NAME} \\\n",
        "  --text_column {TEXT_COLUMN} \\\n",
        "  --lang_code {LANGUAGE_CODE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpoGeFwSEfVt"
      },
      "source": [
        "- Congratulations. You now trained sentiment classifier and predict on the unlabelled tweets.\n",
        "\n",
        "- The prediction file (pred_{lang}.tsv) is in \"afrisenti-semval-2023\" folder under \"submissions\" folder. For example, since we trained hausa (ha), you will be able to see pred_ha.tsv file ready for submission. The submission file is in the format below:\n",
        "\n",
        "<center>\n",
        "\n",
        "|ID | label |\n",
        "|--- | --- |\n",
        "|hau_dev_00001| negative |\n",
        "|hau_dev_00002| positive |\n",
        "|... | ... |\n",
        "\n",
        "</center>\n",
        "\n",
        "- Inside the same folder, you will also see a file \"ha_predictions.tsv\" with the format below to see tweets with corresponding sentiment predictions. This file is not for submission.\n",
        "\n",
        "\n",
        "<center>\n",
        "\n",
        "|ID | text | label |\n",
        "|--- | --- | --- | \n",
        "|hau_dev_00001| @user Allah Miki albarkah 🙏🙏🙏 |  positive |\n",
        "|hau_dev_00002| @user Kidan ma zai dadi😂\t |  negative |\n",
        "|... | ... | ... |\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DObkW3ulM7yg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
