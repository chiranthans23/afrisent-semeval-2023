{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tokenizers\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_NAME = \"Davlan/afro-xlmr-small\"\n",
    "MODEL_NAME = \"Davlan/afro-xlmr-small\"\n",
    "TARGET_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AfriSentiDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "        Holds the dataset and also does the tokenization part\n",
    "    '''\n",
    "    def __init__(self, df, max_len=300):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = 'label' in df\n",
    "        self.labels = {'positive': 0, 'neutral':1, 'negative':2}\n",
    "        self.tokenizer =  BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case = True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.df.iloc[index]\n",
    "\n",
    "        text = data_row.text\n",
    "\n",
    "        #print(f\"label: {data_row.label}, int: {self.labels[data_row.label]}\")\n",
    "        label = self.labels[data_row.label]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            text=text,\n",
    "            input_ids=encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "            label=label\n",
    "        )\n",
    "        \n",
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        AfriSentiDataset(train_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        AfriSentiDataset(val_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        AfriSentiDataset(df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "class AfriSentiModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(AfriSentiModel, self).__init__()\n",
    "        \n",
    "        # config = AutoConfig.from_pretrained(\n",
    "        #     model_name,\n",
    "        #     #use_auth_token = None\n",
    "        #     )    \n",
    "        self.model = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(768, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _,po = self.model(input_ids, attention_mask, return_dict = False)\n",
    "        x = self.dropout(po)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "          \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn():\n",
    "    '''\n",
    "        calculates the loss use CE loss function\n",
    "    '''\n",
    "    return nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3284"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"num_epochs\": 5,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"optimizer\": optim.AdamW,\n",
    "    'model_name': MODEL_NAME,\n",
    "    'folds': 5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 796/796 [19:12<00:00,  1.45s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3856/511257916.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilingual_train.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3856/511257916.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;34m|\u001b[0m \u001b[0mTrain\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtotal_acc_train\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.3\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;34m|\u001b[0m \u001b[0mVal\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtotal_loss_val\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.3\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     | Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}')\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{config['model_name']}_FOLD{fold}.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(train_data,config):\n",
    "    '''\n",
    "        train the model using config as hyperparameters\n",
    "    '''\n",
    "\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config['folds'], shuffle=True, random_state=seed)\n",
    "    train_data['text'] = train_data['text'].astype(str)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_data, train_data.label), start=1): \n",
    "        \n",
    "        print(f'Fold: {fold}')\n",
    "        model = AfriSentiModel(config['model_name'])\n",
    "        optimizer = config['optimizer'](model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999))\n",
    "        criterion = loss_fn()   \n",
    "        dataloaders_dict = get_train_val_loaders(train_data, train_idx, val_idx, config['batch_size'])\n",
    "\n",
    "        train_dataloader, val_dataloader = dataloaders_dict['train'], dataloaders_dict['val']\n",
    "\n",
    "        if use_cuda:\n",
    "                model = model.cuda()\n",
    "                criterion = criterion.cuda()\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        for epoch_num in range(config['num_epochs']):\n",
    "\n",
    "                total_acc_train = 0\n",
    "                total_loss_train = 0\n",
    "                accum_iter = 8\n",
    "\n",
    "                for b_id, td in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "                    train_label = td['label'].to(device)\n",
    "                    mask = td['attention_mask'].to(device)\n",
    "                    input_id = td['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        output = model(input_id, mask)\n",
    "                        batch_loss = criterion(output, train_label.long())\n",
    "                        total_loss_train += batch_loss.item()\n",
    "                        \n",
    "                        acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                        total_acc_train += acc\n",
    "\n",
    "                        model.zero_grad()\n",
    "                        batch_loss /= accum_iter\n",
    "                        batch_loss.backward()\n",
    "\n",
    "                        if ((b_id + 1) % accum_iter == 0) or (b_id + 1 == len(train_dataloader)):\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "                            \n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                total_acc_val = 0\n",
    "                total_loss_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for val_input in val_dataloader:\n",
    "\n",
    "                        val_label = val_input['label'].to(device)\n",
    "                        mask = val_input['attention_mask'].to(device)\n",
    "                        input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                        output = model(input_id, mask)\n",
    "\n",
    "                        batch_loss = criterion(output, val_label.long())\n",
    "                        total_loss_val += batch_loss.item()\n",
    "                        \n",
    "                        acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                        total_acc_val += acc\n",
    "                \n",
    "                print(\n",
    "                    f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader.dataset): .3f} \\\n",
    "                    | Train Accuracy: {total_acc_train / len(train_dataloader.dataset): .3f} \\\n",
    "                    | Val Loss: {total_loss_val / len(val_dataloader.dataset): .3f} \\\n",
    "                    | Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}')\n",
    "                  \n",
    "        torch.save(model.state_dict(), f\"{config['model_name']}_FOLD{fold}.pth\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"multilingual_train.tsv\", sep='\\t', names=['text', 'label'], header=0)\n",
    "train(df,train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(test_data):\n",
    "    ids = test_data.iloc[:,0].astype('str').tolist()\n",
    "    test_data['text'] = test_data['text'].astype(str)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    test_dataloader = get_test_loader(test_data)\n",
    "    predictions = []\n",
    "    models = []\n",
    "\n",
    "    \n",
    "    for fold in range(train_config['folds']):\n",
    "        model = AfriSentiModel(train_config['model_name'])\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load( f\"{train_config['model_name']}_FOLD{fold}.pth\"))\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in test_dataloader:\n",
    "\n",
    "            mask = data['attention_mask'].to(device)\n",
    "            input_id = data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            pred = 0\n",
    "            for model in models:\n",
    "                output = model(input_id, mask)\n",
    "                pred += torch.softmax(output, dim=1).cpu().detach().numpy()\n",
    "\n",
    "            predictions.append(round(pred/train_config['folds']))\n",
    "\n",
    "        labels = pd.Series(predictions).map({0:'positive', 1: 'neutral', 3: 'negative'})\n",
    "\n",
    "    df = pd.DataFrame(list(zip(ids,labels)), columns=['ID', 'label'])\n",
    "    df.to_csv(os.path.join('.', 'pred'+ '.tsv'), sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv(\"multilingual_dev.tsv\", sep='\\t', names=['text'], header=0)\n",
    "infer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
